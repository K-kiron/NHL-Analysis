---
layout: post
title: Milestone 2
---

# <strong>DATA PATH RULES</strong>
## To run the code correctly, please follow the instructions below:<br> 
Rename the data folder to 'IFT6758_Data' and place the folder under the main(father) path <br>
![path_img](../images/path1_m2.png)<br>![path_img](../images/path2_m2.png)<br>

## <strong>1. Experiment Tracking</strong>

### TBD

### See the following parts.

## <strong>2. Feature Engineering I</strong>

### In this milestone, we split the raw data into train, validation and test sets:

![data_split](../images/data_split_m2.png)

### <strong>Question 1</strong>: 

Here we have calculated shot_distance and shot_angle and added them as new columns to the dataframe. And set the number of bins to 20 for plot.<br>
As we can see from the chart, although the number of no-goals is significantly higher than the number of goals, the two roughly follow an approximate distribution.

![fe_q1_1](../images/m2_fe1_q1_1.png)

From the image we can see that the distribution roughly shows a U-shaped distribution with a low centre and two high ends. From the distribution we can guess that due to the large number of people gathered in the middle of the field during the game, it might be difficult to score from this distance, so most of the time the athletes did not perform the shooting action at this distance.

![fe_q1_2](../images/m2_fe1_q1_2.png)

From the image we can see that shot counts show a decreasing trend with increasing angle. This indicates that most of the athletes tend to shoot from the front rather than from a more oblique angle.

![fe_q1_3](../images/m2_fe1_q1_3.png)

From the image we can see that the shot counts have a significant density when the distance is far and the angle is close to 0. From this we can guess that many fast counterattacks (i.e. shots on the opponent's net in front of their own net) took place in the actual match.

### <strong>Question 2</strong>:

In this question instead of histograms, we chose bar charts for plotting. Because for continuous data, the probability of goal at each distance point (or angle point) will only be 1 or 0, the data we get in this case is not meaningful for any study. So we divided the distances and angles into 20 intervals and calculated the goal rate for each interval separately. The results are as follows:

![fe_q2_1](../images/m2_fe1_q2_1.png)

From the image we can see that the distribution roughly shows a U-shaped distribution with a low centre and two high ends. The higher goal rate at closer distances is very understandable, and the lower goal-scoring rate at mid-distance somehow confirms our suspicion in question 1 that there is a large number of players gathered in the middle of the court, which makes it less easy to goal. The high goal rate at longer distances may be due to quick counter-attacks, where the opposing players have no time to react.

![fe_q2_2](../images/m2_fe1_q2_2.png)

From the image we can see that the goal rate is higher when the angle is close to 0, while the other angles have roughly the same goal rate. This is also aligned with our common sense: shots from the front are more likely to goal.

### <strong>Question 3</strong>:

![fe_q3](../images/m2_fe1_q3.png)

From the image we can see that the empty net stays at a very low level no matter what the shot distance is. This shows that in most cases the goalkeepers of both teams stay in front of their own net.<br>
Our domain knowledge is that "it is incredibly rare to score a non-empty net goal on the opposing team from within your defensive zone". But from the image we can see that there are very many non-empty net goals scored from long distance, which is against our domain knowledge and suggests that there may be anomalous data.


## <strong>3. Baseline Models</strong>

### <strong>Question 1</strong>:

The accuracy score of our model on the validation set is approximately 0.906, which means that it correctly predicted about 90.6% of the samples in the validation set.Accuracy is a useful metric when the classes are balanced, but it can be misleading when the class distribution is inbalance.I calculate it by comparing the model's predictions against the actual outcomes in the validation set. It's the number of correct predictions divided by the total number of predictions. 
in summary even  though the accuracy is high in the model but the classifier doesnt work well as our classes are inbalance. this might be due to the feutures that we chosed. goals are not correlated with the distance of the shot from net.

### <strong>Question 2</strong>: 
![q2_2](../images/Q2_1.png)]
The first plot labeled "ROC curve for distance"shows the performance of my classifier in terms of the trade-off between the true positive rate and false positive rate. The area under the curve (AUC) is 0.53, which is slightly better than random guessing (AUC of 0.5). However, an AUC this close to 0.5 indicates that the model does not have a strong discriminatory ability.
![q2_1](../images/Q2_2.png)]
The goal rate (#goals / (#no_goals + #goals)) as a function of the shot probability model percentile, i.e. if a value is the 70th percentile, it is above 70% of the data. 
![q2_3](../images/Q2_3.png)]
third plot is cumulative percentage of goals, which is an empirical cumulative distribution function (ECDF) of the predicted probabilities for the goals. This can help understand the concentration of goals within certain predicted probability ranges.
![q2_4](../images/Q2_4.png)]
 is a calibration curve, it show how well the predicted probabilities of the goals are calibrated. The ideal calibration curve would be a straight line at a 45-degree angle. Deviations from this line indicate over- or under-confidence in predictions. if the model is perfectly calibrated, the predicted probabilities of the positive class would match the actual frequency of the positive class.
### <strong>Question 3</strong>:
![q3_1](../images/3a_ROC_curves.png)]
[Link to Comet Experiment]()
The models trained on angle, distance, and both features outperform the random baseline, as indicated by their AUC values being above 0.50, which is the AUC of a random classifier. However, all AUC values are quite close to the random baseline, suggesting only a marginal improvement over random guessing.
The model that includes both distance and angle as features performs the best, with an AUC of 0.60, indicating that combining features can provide a better prediction capability than using them individually.
![q3_2](../images/3b_goal_rates.png)]
[Link to Comet Experiment]()
This plot shows the goal rates at different shot probability model percentiles. The model trained on both features appears to maintain a consistent advantage over the single-feature models, though there is considerable variability in the goal rates.
All models, including the random baseline, demonstrate a lack of consistency, with goal rates fluctuating across the percentiles, which might indicate that the models are not very reliable predictors of goal probability.
![q3_3](../images/3c_goal_proportions.png)]
[Link to Comet Experiment]()
The cumulative percentage of goals plot shows how well the models rank the shots by their probability of being a goal. Ideally, a higher proportion of actual goals would be found at higher predicted probabilities.
The model using both features again seems to perform better than the others, as it ranks more goals at higher probability percentiles, but the improvement over the single-feature models is not dramatic.
![q3_4](../images/3d_calibration_plots.png)]
[Link to Comet Experiment]()
A well-calibrated model should have points that lie close to the "perfectly calibrated" line, where the predicted probabilities match the observed frequencies.
The calibration plot shows that all models, including the random baseline, are poorly calibrated, with most predictions clustered at the low probability end and not aligning with the diagonal line representing perfect calibration.
Overall, these results suggest that while the logistic regression model benefits from the inclusion of both distance and angle features, the overall predictive performance is only marginally better than random. The models appear to be poorly calibrated, which could result from an overfit to the training data, inadequate feature selection, or inherent unpredictability in the data. To improve the model, it may be necessary to explore more complex models, additional features, or different data preprocessing techniques.

## <strong>4. Feature Engineering II</strong>

### <strong>Question 1</strong>:

### <strong>Question 2</strong>:

### <strong>Question 3</strong>:

### <strong>Question 4</strong>:

### <strong>Question 5</strong>:


## <strong>5. Advanced Models</strong>

### <strong>Question 1</strong>:

### <strong>Question 2</strong>:

### <strong>Question 3</strong>:


## <strong>6. Give it your best shot!</strong>

### <strong>Question 1</strong>:

### <strong>Question 2</strong>:


## <strong>7. Evaluate on test set</strong>

### <strong>Question 1</strong>:

### <strong>Question 2</strong>:
